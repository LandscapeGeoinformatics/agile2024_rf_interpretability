{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05b231da-2370-4660-a7e1-c6dbb928dd38",
   "metadata": {
    "id": "05b231da-2370-4660-a7e1-c6dbb928dd38",
    "tags": []
   },
   "source": [
    "<div style=\"text-align: left\"> Virro, H., Jemeljanova, M., Chan, W.T., Kmoch, A., Uuemaa, E. </div>\n",
    "<div style=\"text-align: left\"> Department of Geography, University of Tartu </div>\n",
    "<div style=\"text-align: left\"> <a href=\"https://landscape-geoinformatics.ut.ee/\">https://landscape-geoinformatics.ut.ee/</a> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc485d46-5601-465b-bd74-c4f7d99e1334",
   "metadata": {
    "id": "cc485d46-5601-465b-bd74-c4f7d99e1334",
    "tags": []
   },
   "source": [
    "<h1><center>Spatial modelling and interpretability with Random Forest</center></h1>\n",
    "<h3><center>AGILE 2024 CONFERENCE</center></h3>\n",
    "<h3><center>University of Glasgow, June 6 2024</center></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cbd75c4-c366-432b-b798-5a7a1b449e55",
   "metadata": {
    "id": "8cbd75c4-c366-432b-b798-5a7a1b449e55"
   },
   "source": [
    "Machine learning has been increasingly used due to its capabilities to work with large amounts of data, while having minimal assumptions on shape or distribution of variables. However, machine-learning models are often considered a **black-box**, and a lack of interpretability would mean that it is hard to determine if the model has found meaningful and realistic relationships between different phenomena. Thus, **there is a strong need to be able to explain and interpret machine-learning models** to understand the effects and relationships of the underlying modelled processes and used covariates.\n",
    "\n",
    "Recently, **various methods to interpret the relationships** between the covariates and the target variable in machine learning **have been introduced** (see, e.g, Molnar 2024). **In this workshop, we will present such methods**, namely **partial dependence plots** and **Shapley values**, and provide tips on how to make use of these methods to build less biased, better understandable, and more robust models. This workshop will provide hands-on tasks on interpreting a machine learning modelâ€™s results using the Python programming language."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db8a097-a670-429f-844c-1fcbc8b83632",
   "metadata": {
    "id": "4db8a097-a670-429f-844c-1fcbc8b83632"
   },
   "source": [
    "In this workshop, we will use an example of predicting the **total nitrogen** (mg/L) using various environmental variables (topography, soil, land use and land cover, hydrology, agriculture, and climate) from various data sources (see the details in Virro et al. 2022). The total nitrogen data was obtained from the KESE (Estonian Environmental Agency, 2021) environment monitoring system website maintained by the Estonian Environment Agency and was the average value between years 2016 and 2020. This workshop is adapted from the study \"Random forest-based modeling of stream nutrients at national level in a data-scarce region\" authored by Virro et al. (https://doi.org/10.1016/j.scitotenv.2022.156613).\n",
    "\n",
    "We will be using Colab in this workshop, however all the materials, including the .yml file needed to create the environment, are available on the GitHub repository: https://github.com/LandscapeGeoinformatics/agile2024_rf_interpretability\n",
    "\n",
    "### TOC for Jupyter notebook (does not work for Colab):\n",
    "* [0. Install required packages](#install_package)\n",
    "* [1. Dependences/ Library Import](#import_library)\n",
    "* [2. Prepare RF training and test sets](#Data_preparation)\n",
    "* [3. Hyperparameter tuning](#HPO)\n",
    "* [4. Model Training (with model parameters based on Hyperparameter tuning)](#HPO_Train)\n",
    "* [5. Calculate SHAP values](#shap_values)\n",
    "* [6. Reduce the number of features based on SHAP values](#reduce_model)\n",
    "* [7. SHAP analysis of the new model](#reduce_model_shap)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "E6D899GxtV7x",
   "metadata": {
    "id": "E6D899GxtV7x"
   },
   "source": [
    "# 0. Install required packages <a class=\"anchor\" id=\"install_package\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d384d72-c11d-454e-9630-09689732114b",
   "metadata": {
    "id": "8d384d72-c11d-454e-9630-09689732114b"
   },
   "source": [
    "First, we will install the required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rso_xIhQtHHR",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rso_xIhQtHHR",
    "outputId": "d5c0b678-83a3-4e9d-bb79-fa80b0011292"
   },
   "outputs": [],
   "source": [
    "# Get the requirement file from github repo\n",
    "!wget -c https://raw.githubusercontent.com/LandscapeGeoinformatics/agile2024_rf_interpretability/main/workshop_materials/requirements.txt\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0801853-8242-4b3b-b59c-310c376e87d4",
   "metadata": {
    "id": "f0801853-8242-4b3b-b59c-310c376e87d4"
   },
   "source": [
    "# 1. Dependences/ Library Import <a class=\"anchor\" id=\"import_library\"></a>\n",
    "Let's import the packages. For the data processing, we will use **numpy** and **pandas**, to read in geospatial data, **geopandas** will be used. **scikit-learn** will be used for Random Forest modelling, **shap** for shap value calculation and plotting.The partial dependence plots will be generated with scikit-learn's **PartialDependenceDisplay** function. **Seaborn** and **matplotlib** will be used for plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d1fd0b4-83ea-4861-8572-6a2dc80f0031",
   "metadata": {
    "id": "1d1fd0b4-83ea-4861-8572-6a2dc80f0031",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from joblib import dump, load\n",
    "from sklearn.metrics import r2_score\n",
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import random\n",
    "from sklearn.inspection import PartialDependenceDisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b13f6b53-0ad7-480a-a38b-725848a365c8",
   "metadata": {
    "id": "b13f6b53-0ad7-480a-a38b-725848a365c8",
    "tags": []
   },
   "outputs": [],
   "source": [
    "sns.set_theme()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8db57b3-c22a-47fc-b1a9-b088899ec02e",
   "metadata": {
    "id": "a8db57b3-c22a-47fc-b1a9-b088899ec02e"
   },
   "source": [
    "Random Forest relies on randomisation (as the name implies) in subsetting data for decision trees. We set the seed so that each re-run of the Random Forest models results in the same random state and thus, the same outcome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75378891-ef6d-44a8-a57a-53e493ad24c3",
   "metadata": {
    "id": "75378891-ef6d-44a8-a57a-53e493ad24c3"
   },
   "outputs": [],
   "source": [
    "# Set seed\n",
    "random_seed = 0\n",
    "random_state = np.random.seed(random_seed)\n",
    "random.seed(random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fdba256-a34c-4806-9d2c-61dce57387e3",
   "metadata": {
    "id": "2fdba256-a34c-4806-9d2c-61dce57387e3"
   },
   "source": [
    "# 2. Prepare RF training and test sets <a class=\"anchor\" id=\"Data_preparation\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c99294-478b-42a0-8cf9-c3a280b20639",
   "metadata": {
    "id": "96c99294-478b-42a0-8cf9-c3a280b20639"
   },
   "source": [
    "The geopackage (*gpkg*) with the prediction target (total nitrogen) and feature values (land use, soil etc) is located in a GitHub repository. Let's download it directly from there.\n",
    "\n",
    "`wget` is a command line function that is used to retrieve data from web servers. We use exclamation mark in front to open a shell and close it immediatelly after the function is executed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cdb9077-17fa-4dd8-867d-66e63b360be5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6cdb9077-17fa-4dd8-867d-66e63b360be5",
    "outputId": "054d0eeb-ba84-4843-8e44-20cd5f9648fb",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Getting the gpkg from repo\n",
    "!wget -c https://github.com/LandscapeGeoinformatics/agile2024_rf_interpretability/raw/main/workshop_materials/agile2024_tn_mean_obs_sites.gpkg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b3aa27b-a255-45a9-98bf-e63b9cacbac8",
   "metadata": {
    "id": "7b3aa27b-a255-45a9-98bf-e63b9cacbac8"
   },
   "source": [
    "## 2.1 Data Exploration (very simple one)\n",
    "Let's explore the contents of the file. We will use the **geopandas** package, since has the capacity to read in and work with spatial data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a25bca-2d45-4ccf-8083-6d401b14ec9d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 790
    },
    "id": "47a25bca-2d45-4ccf-8083-6d401b14ec9d",
    "outputId": "60c933af-c36c-4c4c-b2d8-2ea4449696e8",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Read observation data\n",
    "fp = \"agile2024_tn_mean_obs_sites.gpkg\"\n",
    "obs_data = gpd.read_file(fp)\n",
    "display(obs_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d24a10b4-5293-4214-8bca-41208b4f43e0",
   "metadata": {
    "id": "d24a10b4-5293-4214-8bca-41208b4f43e0"
   },
   "source": [
    "As we can see, the file contains a site code (site_code) and an id (obs_id) for each sampling point, the prediction target as total nitrogen (mg/l)\n",
    "(obs_value), as well as 82 features, and the geometry, where the spatial information is stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f0131c-bf7a-45a6-a4d5-74fc74b35004",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "23f0131c-bf7a-45a6-a4d5-74fc74b35004",
    "outputId": "8abe772e-790f-4636-daff-c5f8b895b6b6",
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "obs_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a08fb8-7eb3-4a04-9dbe-b96813d56061",
   "metadata": {
    "id": "19a08fb8-7eb3-4a04-9dbe-b96813d56061"
   },
   "source": [
    "## 2.2 Data visualization\n",
    "Let's plot the data spatially, using the observed value of total nitrogen (mg/L)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe699137-48bc-40f9-90ce-8e3eccc1efa4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 600
    },
    "id": "fe699137-48bc-40f9-90ce-8e3eccc1efa4",
    "outputId": "6ffdeadc-e5de-4c4c-cc74-515bb7b3e51f"
   },
   "outputs": [],
   "source": [
    "# Create interactive plot of observation values\n",
    "obs_data.explore(\n",
    "    column=\"obs_value\",\n",
    "    cmap=\"YlOrRd\",\n",
    "    tooltip=[\"site_code\", \"obs_value\"],\n",
    "    marker_kwds={\"radius\": 4},\n",
    "    style_kwds={\"color\": \"black\", \"weight\": 1, \"fillOpacity\": 0.9}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66391069-494b-4797-ba8f-112f2a4a9b9a",
   "metadata": {
    "id": "66391069-494b-4797-ba8f-112f2a4a9b9a"
   },
   "source": [
    "## 2.3 Training and Testing Dataset preparation\n",
    "Now, we will extract the features (that will be used to predict the target) and the prediction target in seperate variables. From the training data, we will remove the id columns the prediction target (first three columns) and the geometry column (the last column), as they are not necessary in the model training. We will seperately save the prediction target values. **Throughout the workshop, we will use X to represent covariates (or features), and Y as the target variable.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c9e8551-4eee-4032-b68a-d2eae4354ef0",
   "metadata": {
    "id": "6c9e8551-4eee-4032-b68a-d2eae4354ef0",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Extract features and target\n",
    "X = obs_data.iloc[:, 3:-1]\n",
    "y = obs_data[\"obs_value\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28386212-3ed8-42ac-9931-cf58a46f4d62",
   "metadata": {
    "id": "28386212-3ed8-42ac-9931-cf58a46f4d62"
   },
   "source": [
    "To test the model generalizability, we will split the dataset randomly into 70% of training data and 30% testing data. We will immediatelly create four different dataframes for training and testing to ensure that the features correspond to the respective prediction targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "515ab99a-a549-4502-b678-f63588b60e1c",
   "metadata": {
    "id": "515ab99a-a549-4502-b678-f63588b60e1c",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Split the data into training and test sets\n",
    "test_size = 0.3\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61595388-6dbe-4be3-b2a4-240e87fb4982",
   "metadata": {
    "id": "61595388-6dbe-4be3-b2a4-240e87fb4982"
   },
   "source": [
    "We will train our model on 167 samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d39c71-70a0-4748-8751-74239c3c0283",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 443
    },
    "id": "04d39c71-70a0-4748-8751-74239c3c0283",
    "outputId": "a15cd23c-0682-43a7-b374-490e58ff192a"
   },
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e9d3f5-80f4-46f3-b0a1-6fd5e0eea531",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a2e9d3f5-80f4-46f3-b0a1-6fd5e0eea531",
    "outputId": "03b65d07-5d3d-417b-856d-6ed896c6a79b"
   },
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b387ae21-0016-4e4f-aa9a-64c3137a2629",
   "metadata": {
    "id": "b387ae21-0016-4e4f-aa9a-64c3137a2629"
   },
   "source": [
    "# 3. Hyperparameter tuning <a class=\"anchor\" id=\"HPO\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "267def5a-dcc9-4d3a-9063-ae87d842f69f",
   "metadata": {
    "id": "267def5a-dcc9-4d3a-9063-ae87d842f69f"
   },
   "source": [
    "Usually, the default parameters provided in the scikit-learn package yield acceptable results. However, depending on the data, a substantially higher performance can be achieved with hyperparameter optimisation and hyperparameter tuning is **very important to avoid overfitting**. It is recommended to start with the automatic methods (e.g., grid search or random search) and then manually check some parameters.\n",
    "\n",
    "Today, we will use the RandomisedSearchCV (Bergstra and Bengio, 2012) algorithm. We will define a list of possible hyperparameter values (value ranges), and the algorithm will determine which combination of them performs the best.\n",
    "\n",
    "We will perform 100 runs (`n_iter=100`), and the best hyperparameter combination will be the one with the smallest resulting mean squared error (MSE) when running a model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d1dee66-6435-4d13-9f87-2c8235812645",
   "metadata": {
    "id": "5d1dee66-6435-4d13-9f87-2c8235812645"
   },
   "source": [
    "We will tune the following parameters:\n",
    "\n",
    "* `n_estimators` (number of trees in random forest);\n",
    "\n",
    "* `max_depth` (maximum number of levels in a tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43704a8-78a4-41d0-8beb-2be2a4abaeb7",
   "metadata": {
    "id": "d43704a8-78a4-41d0-8beb-2be2a4abaeb7"
   },
   "outputs": [],
   "source": [
    "# Search for hyperparameters\n",
    "def search_hyperparams(X, y, random_state):\n",
    "\n",
    "    # Number of trees in random forest - test between 50 to 200 with testing interval of 25\n",
    "    step = 25\n",
    "    n_estimators = list(np.arange(start=50, stop=200 + step, step=step))\n",
    "\n",
    "    # Maximum number of levels in a tree - test between 5 and 20 with testing interval of 5\n",
    "    step = 5\n",
    "    max_depth = list(np.arange(start=5, stop=20 + step, step=step))\n",
    "\n",
    "    # Create dictionary from parameters\n",
    "    param_distributions = {\n",
    "        \"n_estimators\": n_estimators,\n",
    "        \"max_depth\": max_depth\n",
    "    }\n",
    "\n",
    "    # Perform search for hyperparameters\n",
    "    estimator = RandomForestRegressor()\n",
    "    rf_random = RandomizedSearchCV(\n",
    "        estimator=estimator, param_distributions=param_distributions, n_iter=100, verbose=2, random_state=random_state,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    rf_random.fit(X, y)\n",
    "\n",
    "    # Get best parameters\n",
    "    params = rf_random.best_params_\n",
    "    params[\"oob_score\"] = True\n",
    "\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d2f927-d27a-4827-8246-151e92ffeab5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "82d2f927-d27a-4827-8246-151e92ffeab5",
    "outputId": "2d8ed867-b3a6-44e8-c783-c066106b434c"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Perform search for hyperparameters\n",
    "params = search_hyperparams(X_train, y_train, random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jyGI58vpYY6H",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jyGI58vpYY6H",
    "outputId": "a45230df-11b8-4636-a386-555dca08b78d"
   },
   "outputs": [],
   "source": [
    "# Show the best hyperparameters\n",
    "params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f26ee4-1e35-4f5a-b5e8-ae60207b1771",
   "metadata": {
    "id": "f8f26ee4-1e35-4f5a-b5e8-ae60207b1771"
   },
   "source": [
    "# 4. Model Training (with model parameters based on Hyperparameter tuning) <a class=\"anchor\" id=\"HPO_Train\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c43a70-8e9d-466e-bc35-f4a3cdde66b7",
   "metadata": {
    "id": "21c43a70-8e9d-466e-bc35-f4a3cdde66b7"
   },
   "source": [
    "Now that we have the optimal hyperparameters, let's train our model using them. First, we define the algorithm we will use (Random Forest) and set the hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98991863-c21d-4eda-b244-f4e80a22e975",
   "metadata": {
    "id": "98991863-c21d-4eda-b244-f4e80a22e975"
   },
   "outputs": [],
   "source": [
    "# RF regressor\n",
    "regressor = RandomForestRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e5fd29-23a8-400f-bd9d-4cab54af6e3e",
   "metadata": {
    "id": "55e5fd29-23a8-400f-bd9d-4cab54af6e3e"
   },
   "outputs": [],
   "source": [
    "# Set hyperparameters\n",
    "regressor = RandomForestRegressor(**params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62392ef2-d698-4b8c-b60b-10029628113d",
   "metadata": {
    "id": "62392ef2-d698-4b8c-b60b-10029628113d"
   },
   "source": [
    "Next, we train the model on our covariate and prediction target data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da9f7e36-c05e-4638-9aaf-eb29030988ca",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 74
    },
    "id": "da9f7e36-c05e-4638-9aaf-eb29030988ca",
    "outputId": "27b25a2c-b1d2-4e0b-bd93-8e8c0182a211"
   },
   "outputs": [],
   "source": [
    "# Fit model\n",
    "regressor.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42239079-5ccc-4a9b-9dc9-b177b12dab33",
   "metadata": {
    "id": "42239079-5ccc-4a9b-9dc9-b177b12dab33"
   },
   "source": [
    "Let's see how well our model performed on the training data by calculating the R squared value. As R squared values span [0;1], where 1 means an ideal fit between the observations and predicted values, and the value goes down as the model predication get worse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910c70fd-86c8-467d-86e2-1d2c1583f5c1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "910c70fd-86c8-467d-86e2-1d2c1583f5c1",
    "outputId": "12ce7804-754e-4b4f-c95e-b102c29c8683",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Calculate accuracy on training set\n",
    "regressor.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79948072-a4c9-40b7-8698-2f7b5334e1b3",
   "metadata": {
    "id": "79948072-a4c9-40b7-8698-2f7b5334e1b3",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Predict\n",
    "Y_train_pred = regressor.predict(X_train)\n",
    "Y_test_pred = regressor.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c48b838-df59-412d-b1cd-0a9a7beba3fa",
   "metadata": {
    "id": "8c48b838-df59-412d-b1cd-0a9a7beba3fa"
   },
   "source": [
    "Let's also determine the model generalisation capacity by determining R squared on independent data that the model has not seen (i.e., our test data). While the score is fair, it is lower than for the training data, meaning that there has been a fair bit of overfitting to the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b913615-0cc6-4ffe-832d-dc7fdded29f2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6b913615-0cc6-4ffe-832d-dc7fdded29f2",
    "outputId": "2c616b3d-2f62-4400-a8b1-663efc36608b",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Calculate accuracy on test set\n",
    "r2_score(y_test, Y_test_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce8f4fb5-6762-49d9-88f7-c19621cd6ecf",
   "metadata": {
    "id": "ce8f4fb5-6762-49d9-88f7-c19621cd6ecf"
   },
   "source": [
    "# 5. Calculate SHAP values <a class=\"anchor\" id=\"shap_values\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be687724-82e4-48cf-88dc-5fe7a7ce8722",
   "metadata": {
    "id": "be687724-82e4-48cf-88dc-5fe7a7ce8722"
   },
   "source": [
    "SHAP (SHapely Additive exPlanations) is a method that originally comes from cooperative game theory, where it was used to determine each player's contribution on the resulting game score. Similarly, in machine learning, it is used to quantify the contribution of each covariate on the prediction target in an additive, linear manner (meaning that the contributions of features are summed). The SHAP are used to reveal if the contribution is positive or negative, and to quantify it. The values are in the units of the prediction target (in our case, mg/L), which makes them easy to interpret. Read more about SHAP in Molnar 2024 and https://shap.readthedocs.io/en/latest/.\n",
    "\n",
    "The SHAP value method is model-agnostic, meaning that it can be used on various machine learning algorithms. For our Random Forest models, we will use the TreeSHAP (function **TreeExplainer**), which is a variant of SHAP made specifically for decision tree models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592f1aeb-4c5d-4067-a973-41d94175cd78",
   "metadata": {
    "id": "592f1aeb-4c5d-4067-a973-41d94175cd78"
   },
   "outputs": [],
   "source": [
    "# Calculate SHAP values\n",
    "explainer = shap.TreeExplainer(regressor)\n",
    "shap_values = explainer.shap_values(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f8373f-0cf7-42cc-b074-085586596422",
   "metadata": {
    "id": "36f8373f-0cf7-42cc-b074-085586596422"
   },
   "source": [
    "Shap values are organised in an array of arrays by sample and then by the covariate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07be543c-c761-46fc-89d4-615dbe4b0977",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "07be543c-c761-46fc-89d4-615dbe4b0977",
    "outputId": "b78f62e6-a1ba-44f7-de5d-7ef18de5f087",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "shap_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a901e473-5701-4570-b888-18faff191730",
   "metadata": {
    "id": "a901e473-5701-4570-b888-18faff191730"
   },
   "source": [
    "The SHAP values can be calculated both globally (for the whole dataset) and locally (distinguishing individual instances). First, let's plot SHAP globally using the summary plot.\n",
    "\n",
    "This plot shows the **absolute contribution of each feature**. The features are ordered based on their importance, and here, 20 out of 82 most important features are plotted. On average, the highest contribution to the total nitrogen value is by the proportion of arable land in catchment (arable_prop), features mean rock content of the first layer (rock1_mean), silt % of the first soil layer (silt1_mean) and rip_veg_nat_prop (total area of riparian vegetation buffer around natural streams divided by catchment area), all exceeding 0.1 mg/L. All other features contributed by 0.1 mg/L or less.\n",
    "\n",
    "**NB!** While SHAP might seem similar to permutation based feature importance, the latter describes feature contribution to the prediction accuracy, not to the target value! Please be mindful when interpreting the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a85bbcab-2385-41ca-bcf0-11cfab5c5298",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 968
    },
    "id": "a85bbcab-2385-41ca-bcf0-11cfab5c5298",
    "outputId": "7f7d1d16-7800-40cf-9601-0935eafd0bf7"
   },
   "outputs": [],
   "source": [
    "# SHAP summary bar plot\n",
    "shap.summary_plot(shap_values=shap_values, features=X_train, feature_names=X_train.columns, plot_type=\"bar\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e9610a5-644f-472c-8917-d152f2842d08",
   "metadata": {
    "id": "2e9610a5-644f-472c-8917-d152f2842d08"
   },
   "source": [
    "For a more thorough exploration, it is always advisable to plot the **summary beeswarm plot**, where individual instances can be observed. The interpretation of the summary beeswarm plot is as follows: each point is the SHAP value in a distinct sampling location. For each feature, there are as many points as sampling locations in our dataset. The color describes the covariate value relative to the range of covariate values in the dataset, for each covariate seperately. The red dots represent high values (for example, high rock content in the first layer), while blue represents low values.\n",
    "\n",
    "The impact on the target value (in our case, total nitrogen mg/L) can be read from the x axis: the more a certain point is positioned to the right, the higher the contribution, and vice versa; for example, high arable proportion in the catchment (arable_prop) results in up to 0.6 mg/L higher total nitrogen concentration in certain locations.\n",
    "\n",
    "Besides that, we can see that features like rock content in the first layer (rock1_mean) and the arable proportion in the catchment (arable_prop) contribute positively to the target, since higher feature values result in higher contribution, while features like k1_mean (mean hydraulic conductivity of the first layer) and rip_veg_nat_prop (total area of riparian vegetation buffer around natural streams divided by catchment area) have a negative influence: lower values result in higher total nitrogen concentration.\n",
    "\n",
    "As you can see, the contribution of high vs. low values of feature is not symmetric. For example, low values of rip_veg_nat_prop contribute up to 2.6 mg/L increase on the target, while high values of the same feature decrease the total nitrogen content by only 0.6 mg/L at most.\n",
    "\n",
    "**SHAP values and partial dependence plots (below) are a great tool of model validation: the modeler can see if the relationships revealed by the model are logical and correspond to the domain knowledge.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d633d10-4ae2-4826-a259-1035b9939469",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 968
    },
    "id": "5d633d10-4ae2-4826-a259-1035b9939469",
    "outputId": "f81b7375-b1a0-4689-e8f4-3ff107afe014"
   },
   "outputs": [],
   "source": [
    "# SHAP summary beeswarm plot\n",
    "shap.summary_plot(shap_values=shap_values, features=X_train, feature_names=X_train.columns)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d216979-fe77-4039-a9b4-6c39e0876c39",
   "metadata": {
    "id": "9d216979-fe77-4039-a9b4-6c39e0876c39"
   },
   "source": [
    "To understand better how each predictor is contributing to the model and how it is related to the target variable, we can use partial dependence plots. We don't want to plot all 82 features and therefore we will subset the 10 most important covariates based on SHAP values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f07b4f81-adf4-4005-bd6f-b7cf9668607d",
   "metadata": {
    "id": "f07b4f81-adf4-4005-bd6f-b7cf9668607d"
   },
   "source": [
    "## 5.1 Top 10 features with the highest SHAP value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d3e9cf6-1b01-4f54-851d-b70541f167d5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "id": "1d3e9cf6-1b01-4f54-851d-b70541f167d5",
    "outputId": "ebfc4591-fd09-41d7-c6c6-8b37103a2efe"
   },
   "outputs": [],
   "source": [
    "#extracting top 10 features based on SHAP values\n",
    "feature_names = X_train.columns\n",
    "rf_resultX = pd.DataFrame(shap_values, columns = feature_names)\n",
    "vals = np.abs(rf_resultX.values).mean(0)\n",
    "shap_importance = pd.DataFrame(list(zip(feature_names, vals)),\n",
    "                                  columns=[\"feature\",\"feature_importance_vals\"])\n",
    "shap_importance.sort_values(by=[\"feature_importance_vals\"],\n",
    "                               ascending=False, inplace=True)\n",
    "shap_importance[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b209f96-b2d1-4295-a6d7-ecb1e7a417fa",
   "metadata": {
    "id": "5b209f96-b2d1-4295-a6d7-ecb1e7a417fa"
   },
   "source": [
    "## 5.2 Plot the partial dependence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74022e6f-c168-4e29-88f6-db742a9e709e",
   "metadata": {
    "id": "74022e6f-c168-4e29-88f6-db742a9e709e"
   },
   "source": [
    "Partial Dependence plots show the marginal effect of feature on a target in a ML model, that is, the contribution of a specific feature on the target after the average impacts of other features have been removed. Usually, partial dependence plots are drawn for one to two features at time, as the interpretation of more features becomes increasingly complex.\n",
    "\n",
    "With these plots, one can determine if the influence of feature on the prediction target is linear or non-linear. Moreover, the x axis shows the values of the respective feature, while the y axis visualises the target values (in our case, total nitrogen, mg/L). Thus, these plots are intuitive to understand and interpret and certain thresholds of influence can be determined. For example, with an increasing proportion of arable land (feature \"arable_prop\"), the concentration of total nitrogen increases. A steep increase is noted when the arable proportion exceeds 40%, raising from around 2.5 mg/L to around 4.5 mg/L. As with SHAP, these plots are a great validation tool. New insights can also be gained, however caution should be exercised when interpreting the relationships. For example, keep in mind that the marks of data indicate value density around certain values in your dataset, therefore a lower certainty is in the data range with low number of points (e.g., range 20-40 vs. range 60-80 for the feature k1_mean)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24289542-b3cb-46b4-8668-df9122562738",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 977
    },
    "id": "24289542-b3cb-46b4-8668-df9122562738",
    "outputId": "fde86a75-db40-49b2-c6d4-dc497a441c2b"
   },
   "outputs": [],
   "source": [
    "# Create partial dependence plot (Full Model)\n",
    "fig, ax = plt.subplots(figsize=(12, 12))\n",
    "disp = PartialDependenceDisplay.from_estimator(regressor, X_train, shap_importance[:10][\"feature\"].values, ax=ax)\n",
    "fig.subplots_adjust(hspace=0.3)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2630e1e3-6044-472c-a5ac-e1573a48fc5a",
   "metadata": {
    "id": "2630e1e3-6044-472c-a5ac-e1573a48fc5a"
   },
   "source": [
    "# 6. Reduce the number of features based on SHAP values <a class=\"anchor\" id=\"reduce_model\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc3462a6-cda8-4334-8d85-89e80215f621",
   "metadata": {
    "id": "dc3462a6-cda8-4334-8d85-89e80215f621"
   },
   "source": [
    "As a general rule, we are interested in models needing as little features as possible due to computational etc. limitations. There are various feature reduction methods, such as permutation based feature importance, forward feature selection, and others. Now we will decrease our feature set using the results of SHAP and will explore the difference in the prediction accuracy.\n",
    "\n",
    "We will use the 10 most influential features (from the original 82) as determined by SHAP and will re-train the Random Forest model with this decreased feature set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91334238-5cd4-4dd6-8374-9eb672169b0f",
   "metadata": {
    "id": "91334238-5cd4-4dd6-8374-9eb672169b0f"
   },
   "outputs": [],
   "source": [
    "abs_mean_shap_df = shap_importance[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c18c4c-dee2-4688-ae3d-c253be92f11e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b3c18c4c-dee2-4688-ae3d-c253be92f11e",
    "outputId": "f2c0f8c9-898d-4a8b-d4da-f4fa502c4664"
   },
   "outputs": [],
   "source": [
    "# List of most important features\n",
    "n_features = 10\n",
    "top_features = abs_mean_shap_df[\"feature\"].head(n_features).to_list()\n",
    "print(top_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb05b7b-a3ee-4d15-acb5-7b55cd945206",
   "metadata": {
    "id": "cbb05b7b-a3ee-4d15-acb5-7b55cd945206"
   },
   "source": [
    "We subset the relevant features from the original training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "372adf37-9a23-406a-a42f-01bd26a36310",
   "metadata": {
    "id": "372adf37-9a23-406a-a42f-01bd26a36310"
   },
   "outputs": [],
   "source": [
    "# Generate new training and test feature sets\n",
    "X_train_reduced = X_train[top_features]\n",
    "X_test_reduced = X_test[top_features]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1502ca4-b6c2-45da-895e-161ec744c6df",
   "metadata": {
    "id": "a1502ca4-b6c2-45da-895e-161ec744c6df"
   },
   "source": [
    "## 6.1 Train model on reduced data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "630b6f3b-c06c-4be0-b19d-32d164e67974",
   "metadata": {
    "id": "630b6f3b-c06c-4be0-b19d-32d164e67974"
   },
   "source": [
    "Again, we will determine the optimal hyperparameters with the exact same methodology as before, but on the reduced training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b8e3cbb-b6dd-4280-a507-d989f1f01560",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8b8e3cbb-b6dd-4280-a507-d989f1f01560",
    "outputId": "658fd9d5-9749-4d14-d3a8-f5af609c4208",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Perform search for hyperparameters\n",
    "params_reduced = search_hyperparams(X_train_reduced, y_train, random_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ef3696-d62f-4eb3-ba84-f6d29f1efe14",
   "metadata": {
    "id": "f5ef3696-d62f-4eb3-ba84-f6d29f1efe14"
   },
   "source": [
    "Let's train the model with the optimal hyperparameters and the reduced training set the same way we did before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d7094e-fda7-4f38-9a12-7dc62afeff52",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 74
    },
    "id": "38d7094e-fda7-4f38-9a12-7dc62afeff52",
    "outputId": "66241b1d-bba0-4e0f-f575-ad41e04943e9"
   },
   "outputs": [],
   "source": [
    "# RF regressor\n",
    "regressor_reduced = RandomForestRegressor()\n",
    "\n",
    "# Set hyperparameters\n",
    "regressor_reduced.set_params(**params_reduced)\n",
    "\n",
    "# Fit model\n",
    "regressor_reduced.fit(X_train_reduced, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ceb4042-d2eb-4b13-9360-de27ad2020cc",
   "metadata": {
    "id": "8ceb4042-d2eb-4b13-9360-de27ad2020cc"
   },
   "source": [
    "As you can see, the accuracy on the training and test set with the lower feature set has remained practically the same with less data needed. Great! Keep in mind though, that this is not a guarantee- the scores can decrease, depending on the data, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86341c35-1269-4985-bb06-8bb8ec0c2053",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "86341c35-1269-4985-bb06-8bb8ec0c2053",
    "outputId": "da2d1ac7-7ef6-4382-ef0e-36cf9bbac957"
   },
   "outputs": [],
   "source": [
    "# Calculate accuracy on training set\n",
    "regressor_reduced.score(X_train_reduced, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc8497f-b094-4ca1-b6a2-5458e8cb93af",
   "metadata": {
    "id": "1dc8497f-b094-4ca1-b6a2-5458e8cb93af",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Predict\n",
    "Y_train_pred_reduced = regressor_reduced.predict(X_train_reduced)\n",
    "Y_test_pred_reduced = regressor_reduced.predict(X_test_reduced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab9adc6-4c85-42d6-b2ef-34a79b2a4a36",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aab9adc6-4c85-42d6-b2ef-34a79b2a4a36",
    "outputId": "648c2284-f7e2-44e7-c7fb-5400e00188d0",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Calculate accuracy on test set\n",
    "r2_score(y_test, Y_test_pred_reduced)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d949223-e922-4190-b63f-96cdf469c43c",
   "metadata": {
    "id": "4d949223-e922-4190-b63f-96cdf469c43c"
   },
   "source": [
    "# 7. SHAP analysis of the new model <a class=\"anchor\" id=\"reduce_model_shap\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae0c8f5-b77d-4448-a20c-f618f304f42a",
   "metadata": {
    "id": "4ae0c8f5-b77d-4448-a20c-f618f304f42a"
   },
   "source": [
    "Now, we will again plot the SHAP values and explore the differences in contribution between the full and the decrease data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff82b1b5-b897-4a88-86db-d0afd62697e2",
   "metadata": {
    "id": "ff82b1b5-b897-4a88-86db-d0afd62697e2"
   },
   "outputs": [],
   "source": [
    "# Calculate SHAP values\n",
    "explainer_reduced = shap.TreeExplainer(regressor_reduced)\n",
    "shap_values_reduced = shap.TreeExplainer(regressor_reduced).shap_values(X_train_reduced)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27915b3e-1192-4f44-afe9-f59ff9f1adae",
   "metadata": {
    "id": "27915b3e-1192-4f44-afe9-f59ff9f1adae"
   },
   "source": [
    "The order has remained the same but the influence has increased a bit. That is a good indicator, showing that the model is quite robust and you have meaningful predictors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f22bb786-ec28-41f6-bace-f6c10429758d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 568
    },
    "id": "f22bb786-ec28-41f6-bace-f6c10429758d",
    "outputId": "68e8cb94-0d8c-4806-ad76-952ef01210ab"
   },
   "outputs": [],
   "source": [
    "# SHAP summary beeswarm plot\n",
    "shap.summary_plot(shap_values=shap_values_reduced, features=X_train_reduced, feature_names=X_train_reduced.columns)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c8638d3-086c-4e5a-97d1-f5eb50f7086b",
   "metadata": {
    "id": "1c8638d3-086c-4e5a-97d1-f5eb50f7086b"
   },
   "source": [
    "Let's look at the partial dependence plots once again. As before, the relationships remain the same. As with SHAP, these plots are a great validation tool. New insights can also be gained, however caution should be exercised when interpreting the relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b1bbeab-5a98-4ba4-8aae-69fab1ff81c1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 977
    },
    "id": "9b1bbeab-5a98-4ba4-8aae-69fab1ff81c1",
    "outputId": "d9d6390e-0366-4a62-a6f0-010a5d6dd910"
   },
   "outputs": [],
   "source": [
    "# Create partial dependence plot\n",
    "fig, ax = plt.subplots(figsize=(12, 12))\n",
    "disp = PartialDependenceDisplay.from_estimator(regressor_reduced, X_train_reduced, X_train_reduced.columns, ax=ax)\n",
    "fig.subplots_adjust(hspace=0.3)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2K8g3bJGRfQ5",
   "metadata": {
    "id": "2K8g3bJGRfQ5"
   },
   "source": [
    "## 7.1 Plot the predictions spatially"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6923db5c-5325-4c75-916e-e5a67ad2e804",
   "metadata": {
    "id": "6923db5c-5325-4c75-916e-e5a67ad2e804"
   },
   "source": [
    "Now, we will plot all predictions spatially (for both train and test locations) of our target value (total nitrogen, mg/L) from our model with the reduced feature set. First, we will create a dataframe with all locations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "CAlLWJxSRjEc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 226
    },
    "id": "CAlLWJxSRjEc",
    "outputId": "f1b1c293-bc0e-4f61-90dd-9eb8174f3216"
   },
   "outputs": [],
   "source": [
    "# Concatenate reduced features and sort by index\n",
    "X_reduced = pd.concat([X_train_reduced, X_test_reduced]).sort_index()\n",
    "X_reduced.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f154bb20-fc04-4561-8962-06e8d51c8fd4",
   "metadata": {
    "id": "f154bb20-fc04-4561-8962-06e8d51c8fd4"
   },
   "source": [
    "Next, we will use our trained model to predict the target values for all locations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "RQfcrfZKTKjH",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 850
    },
    "id": "RQfcrfZKTKjH",
    "outputId": "8986278f-77c5-40df-ce0e-7a1075af1d21"
   },
   "outputs": [],
   "source": [
    "# Predict with the reduced model\n",
    "predictions = regressor_reduced.predict(X_reduced)\n",
    "display(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "KHvYwHTbTdSa",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 429
    },
    "id": "KHvYwHTbTdSa",
    "outputId": "9acd1998-1895-4f5f-b6e3-8dbd0aaf80f8"
   },
   "outputs": [],
   "source": [
    "# Add predictions to observation data\n",
    "obs_data_pred = obs_data.copy()\n",
    "obs_data_pred[\"pred_value\"] = predictions\n",
    "obs_data_pred.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ade345-66fa-4377-b3f6-c78924ed25bb",
   "metadata": {
    "id": "90ade345-66fa-4377-b3f6-c78924ed25bb"
   },
   "source": [
    "Now, we will plot our predicted values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "CcmtgeW2VtAH",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 600
    },
    "id": "CcmtgeW2VtAH",
    "outputId": "86ff38e2-096f-4fb0-fb50-9cda11507a41"
   },
   "outputs": [],
   "source": [
    "# Create interactive plot of predicted values\n",
    "obs_data_pred.explore(\n",
    "    column=\"pred_value\",\n",
    "    cmap=\"YlOrRd\",\n",
    "    tooltip=[\"site_code\", \"obs_value\", \"pred_value\"],\n",
    "    marker_kwds={\"radius\": 4},\n",
    "    style_kwds={\"color\": \"black\", \"weight\": 1, \"fillOpacity\": 0.9}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e4ff79-b2f7-4820-8776-eccd2bbd8591",
   "metadata": {
    "id": "68e4ff79-b2f7-4820-8776-eccd2bbd8591"
   },
   "source": [
    "**Residuals** are the difference between the observed and predicted values. If residuals are spatially autocorrelated (meaning that values in close vicinity are similar), it indicates that there might be a feature missing. This information can be used to further improve the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07K8shWCV2W1",
   "metadata": {
    "id": "07K8shWCV2W1"
   },
   "outputs": [],
   "source": [
    "# Calculate residuals between observed and predicted values\n",
    "obs_data_pred[\"residual\"] = obs_data_pred[\"obs_value\"] - obs_data_pred[\"pred_value\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vRtt65f0WFrq",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 600
    },
    "id": "vRtt65f0WFrq",
    "outputId": "26d356bb-37ce-4b58-fd70-7ec1dcdc0b46"
   },
   "outputs": [],
   "source": [
    "# Create interactive plot of residuals\n",
    "obs_data_pred.explore(\n",
    "    column=\"residual\",\n",
    "    cmap=\"RdBu_r\",\n",
    "    tooltip=[\"site_code\", \"obs_value\", \"pred_value\", \"residual\"],\n",
    "    marker_kwds={\"radius\": 4},\n",
    "    style_kwds={\"color\": \"black\", \"weight\": 1, \"fillOpacity\": 0.9}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b00a09a-bea2-4d93-805f-1ac4033d5eab",
   "metadata": {
    "id": "4b00a09a-bea2-4d93-805f-1ac4033d5eab"
   },
   "source": [
    "Next, let's explore the observed and predicted values. The plot indicates that the model overestimated the some of the low values and underestimated some of the high values, which is characteristic of Random Forest models. It could be due to the characteristics of how data is subsampled for the decision trees, meaning that the extreme values are not always included in training each tree, therefore the model does not learn all the possible value combinations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4inNPAOOXjqI",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 481
    },
    "id": "4inNPAOOXjqI",
    "outputId": "a0daeb52-c163-4d36-d8c6-4829b7dd2875"
   },
   "outputs": [],
   "source": [
    "# Plot observed and predicted values\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "sns.scatterplot(data=obs_data_pred, x=\"obs_value\", y=\"pred_value\")\n",
    "plt.tight_layout()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
